{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from networkx.algorithms import bipartite\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib\n",
    "font = {'family' : 'DejaVu Sans', 'weight' : 'normal', 'size'   : 22}\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degree(g, nodes=None, as_list=True):\n",
    "    deg = dict(g.degree())\n",
    "    if nodes: deg = dict(g.degree(nodes))\n",
    "    \n",
    "    if as_list: return list(deg.values())\n",
    "    return deg\n",
    "\n",
    "def degree_plot(g, nodes=None, filename=None, title=''):\n",
    "    deg = degree(g, nodes=nodes)\n",
    "    bins = 100\n",
    "    if len(nodes) < 100:\n",
    "        bins = len(nodes)\n",
    "    hist = np.histogram(deg, bins=bins)\n",
    "    freqs, edges = hist[0], hist[1]\n",
    "    n = freqs.size\n",
    "    means = [(edges[i] + edges[i+1]) / 2 for i in range(n)]\n",
    "    \n",
    "    # SCATTER PLOT\n",
    "    plt.figure(figsize=[15,10])\n",
    "    plt.plot(means, freqs, \".\", markersize=20)\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"frequency\")\n",
    "    plt.title(\"Degree distribution for %s\" % title)\n",
    "    if filename: plt.savefig('plots/%s.svg' % filename, format='svg', bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    # LOG LOG PLOT\n",
    "    plt.figure(figsize=[15,10])\n",
    "    plt.loglog(means, freqs, \".\", markersize=20)\n",
    "    plt.xlabel(\"log(k)\")\n",
    "    plt.ylabel(\"log(frequency)\")\n",
    "    plt.title(\"Log-log degree distribution for %s\" % title)\n",
    "    if filename: plt.savefig('plots/log_%s.svg' % filename, format='svg', bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "### What is your dataset?\n",
    "The dataset used in this project comes from the data provided for the Yelp Dataset Challenge. This dataset consists of about 1.5 million users and about 200 thousand businesses from North America.  Additionally the dataset includes just under 6 million reviews, made by users of the Yelp service, to businesses. The businesses included in the dataset are both restaurants as well as businesses offering other services, such as postal delivery. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why did you choose this/these particular dataset(s)?\n",
    "The dataset is tremendous\n",
    "### What was your goal for the end user's experience?\n",
    "The purpose of this project is to investigate properties of Yelp’s Elite users. For this paper, the focus will lie on Yelp’s two primary claims about their Elite users:\n",
    "\n",
    "Yelp states that its Elite users have high connectivity, which means that they are connected with many other users and interact often with members of their Yelp community. \n",
    "\n",
    "Yelp claims that its Elite users make up the “true heart of the Yelp community.” Third, Yelp claims that its users have high contribution, which means that the user has made a large impact on the site with meaningful and high-quality reviews. \n",
    "\n",
    "The first goal of our project is to analyze whether the above claims about Yelp’s Elite users are quantifiably valid. For this, we will specify several characteristics which we expect Elite users to have based on these claims. We will then perform analyses on Yelp’s dataset in order to determine whether these properties are truly represented among the Elite users. The secondary goal of our project is to find which properties are most indicative of Elite status on Yelp. \n",
    "\n",
    "The analyses for the first goal can be used for this purpose as well. This kind of information may be useful for those who are interested in becoming Elite members on Yelp. In order to become a member of the “Elite squad,” a user must go through an application process. Despite the suggestions presented above, Yelp doesn’t provide any specific criteria on exactly what characteristics a user must have to become Elite. The mystery behind the selection process for Elite users is well-documented.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic stats. Let's understand the dataset better\n",
    "### Write about your choices in data cleaning and preprocessing\n",
    "- Mis-formatted JSON to valid JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def cleanup(N, dataset, chunk_size=100000):\n",
    "    '''\n",
    "    Cleans up a JSON file by adding a trailing comma to each line,\n",
    "    which is missing from the Yelp dataset files.\n",
    "    A chunk size must be specified, since all the lines in the data\n",
    "    files cannot be stored in memory at the same time, due to being very large!\n",
    "    '''\n",
    "    for k in range(N):\n",
    "        dirty_path = 'yelp_dataset/yelp_academic_dataset_%s.json' % dataset\n",
    "        clean_path = \"cleaned/%s%i.json\" % (dataset, k)\n",
    "        dirty_file = open(dirty_path, \"r\")\n",
    "        clean_file = open(clean_path, \"w\")\n",
    "\n",
    "\n",
    "        start = chunk_size * k\n",
    "        end = chunk_size * (k+1)\n",
    "\n",
    "        content = ''\n",
    "        i = 0\n",
    "        for line in dirty_file:\n",
    "            if i == end:\n",
    "                break\n",
    "            elif i >= start:\n",
    "                s = line.replace('\\n', ',\\n')\n",
    "                content += s\n",
    "            i += 1\n",
    "        if content:\n",
    "            payload = '{\"data\" : \\n[%s]}' % (content[:-2] + '\\n')\n",
    "            clean_file.write(payload)\n",
    "        else:\n",
    "            print(\"No more content.\")\n",
    "    \n",
    "    print('Iteration', k, 'done')\n",
    "    \n",
    "def read_json_to_df(N, dataset):\n",
    "    # Create dataframe from JSON files\n",
    "    df_matrix = [None] * N\n",
    "    for i in range(N):\n",
    "        path = \"cleaned/business%i.json\" % i\n",
    "        df_matrix[i] = pd.DataFrame(list(pd.read_json(path).data))\n",
    "    return pd.concat(df_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all restaurants from Toronto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tnni/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py:4405: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "# Clean business JSON files\n",
    "N = 2 # There are about 200k restaurants, therefore 2 chunks of 100k elements is sufficient\n",
    "dataset = 'business'\n",
    "cleanup(N, dataset)\n",
    "\n",
    "# Make dataframe from JSON data\n",
    "df = read_json_to_df(N, dataset)\n",
    "\n",
    "# Restaurants will contain the keywords 'restaurant' \n",
    "# and/or 'food' in the 'category' attribute.\n",
    "keywords = ['restaurant', 'food']\n",
    "idx = df.categories.str.lower().str.contains(\"|\".join(keywords)).fillna(False)\n",
    "rest = df[idx]\n",
    "\n",
    "\n",
    "# Only include Toronto restaurants\n",
    "rest.city = rest.city.str.lower()\n",
    "rest = rest[rest.city == 'toronto']\n",
    "\n",
    "# Drop attributes irrelevant to the analysis\n",
    "rest = rest.drop(['city', 'attributes', 'categories', 'address', 'neighborhood', 'is_open', 'hours'], axis=1)\n",
    "\n",
    "# Save dataset to CSV\n",
    "rest.to_csv('toronto2/toronto_restaurants.csv', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all reviews from Toronto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-95a5ea671fac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m60\u001b[0m \u001b[0;31m# There are about 6M restaurants, therefore 60 chunks of 100k elements is sufficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'review'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Make dataframe from JSON data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-3786eed98abe>\u001b[0m in \u001b[0;36mcleanup\u001b[0;34m(N, dataset, chunk_size)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdirty_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Clean business JSON files\n",
    "N = 60 # There are about 6M reviews, therefore 60 chunks of 100k elements is sufficient\n",
    "dataset = 'review'\n",
    "cleanup(N, dataset)\n",
    "\n",
    "# Make dataframe from JSON data\n",
    "df = read_json_to_df(N, dataset)\n",
    "\n",
    "# Filter out reviews of businesses outside Toronto\n",
    "reviews = df[df.business_id.isin(rest.business_id)]\n",
    "\n",
    "# Drop attributes irrelevant to the analysis\n",
    "reviews = reviews.drop(['cool', 'funny', 'useful'], axis=1)\n",
    "\n",
    "# Save dataset to CSV\n",
    "reviews.to_csv('toronto2/toronto_reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all users in the Toronto reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean business JSON files\n",
    "N = 30 # A guess\n",
    "dataset = 'user'\n",
    "cleanup(N, dataset)\n",
    "\n",
    "# Make dataframe from JSON data\n",
    "df = read_json_to_df(N, dataset)\n",
    "\n",
    "# Filter out users not in the Toronto reviews\n",
    "toronto_users = df[df.user_id.isin(reviews.user_id)]\n",
    "\n",
    "# Drop attributes irrelevant to the analysis\n",
    "toronto_users = toronto_users.drop(['compliment_cool', 'compliment_cute',\n",
    "       'compliment_funny', 'compliment_hot', 'compliment_list',\n",
    "       'compliment_more', 'compliment_note', 'compliment_photos',\n",
    "       'compliment_plain', 'compliment_profile', 'compliment_writer', 'cool',\n",
    "     'funny', 'fans'], axis=1)\n",
    "\n",
    "# Save to CSV\n",
    "toronto_users.to_csv('toronto/toronto_users.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset stats\n",
    "- Reviews: ~6 million\n",
    "- Users: Many\n",
    "- Businesses: ~200,000\n",
    "\n",
    "For this project the restaurants in Toronto were the main focus, as Toronto is a big city with more than a sufficient amount of data to perform a serious analysis, but small enough for various graph algorithms to be carried out. The users considered in this project were all the users who left a review on a business in Toronto.\n",
    "\n",
    "- Period: March 1st 2008 to August 1st 2018\n",
    "- Reviews: ~380,000\n",
    "- Users: ~85,000\n",
    "- Elite users hereof: ~7,500\n",
    "- Restaurants: ~10,000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools, theory and analysis. Describe the process of theory to insight\n",
    "\n",
    "### Talk about how you've worked with text, including regular expressions, unicode, etc.\n",
    "\n",
    "### Describe which network science tools and data analysis strategies you've used, how those network science measures work, and why the tools you've chosen are right for the problem you're solving.\n",
    "* NetworkX for creating the graphs for analysis. \n",
    "* Centrality measures: Degree and eigenvalue. Degree tells us how popular a user is in terms of friends, or how many reviews they made.\n",
    "* Clustering coefficient: How connected is the graph on average?\n",
    "* Degree distributions: Do the network degrees obey power law distributions?\n",
    "\n",
    "\n",
    "#### Modelling the social network\n",
    "The social network was created by extracting the friends of each user who made a review on a Toronto-based restaurant, and then creating a link between each user. Some users in the social network will not have placed a review on a Toronto-based restaurant, and will only be in the network due to their friendship with someoneone who has. \n",
    "\n",
    "#### Modelling the review network\n",
    "The Toronto Yelp review network was modelled as an undirected graph, containing user nodes where the edges between two user nodes represent the fact two users have reviewed the same restaurant. \n",
    "\n",
    "#### Most connected subcomponent\n",
    "Detecting how important the elite users were for the network was done by removing them from the network in small chunks, and then observing how the largest connected subcomponent shrinks. The elite users were deleted based on their degree centrality.\n",
    "\n",
    "### How did you use the tools to understand your dataset?\n",
    "\n",
    "* NetworkX for creating the graphs for analysis. \n",
    "* Centrality measures: Degree and eigenvalue. Degree tells us how popular a user is in terms of friends, or how many reviews they made.\n",
    "* Clustering coefficient: How connected is the graph on average?\n",
    "* Degree distributions: Do the network degrees obey power law distributions?\n",
    "* Plotting: We tried plotting it, but the network is simply too large.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling the review network\n",
    "The Toronto Yelp review network was modelled as an undirected graph, containing user nodes where the edges between two user nodes represent the fact two users have reviewed the same restaurant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant Strings\n",
    "USER = 'user'\n",
    "ELITE_USER = 'elite_user'\n",
    "BIZ = 'biz'\n",
    "\n",
    "# Read in data\n",
    "biz = pd.read_csv('toronto/toronto_biz.csv')\n",
    "user = pd.read_csv('toronto/toronto_users.csv')\n",
    "reviews = pd.read_csv('toronto/toronto_reviews.csv')\n",
    "elite_user = user[~user.elite.str.contains('None')]\n",
    "\n",
    "print('#Reviews:', len(reviews))\n",
    "print('#Users:', len(set(reviews.user_id)))\n",
    "print('#Elite users:', len(elite_user))\n",
    "print('#Businesses:', len(set(reviews.business_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A node class for storing data.\n",
    "class Node:\n",
    "    def __init__(self, Data, Type):\n",
    "        self.Data = Data\n",
    "        self.Type = Type\n",
    "    \n",
    "    def to_string(self):\n",
    "        return \"Node (%s), Data: \" % (self.Type, self.Data)\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.Data)\n",
    "    def __eq__(self, other):\n",
    "        return (\n",
    "                self.__class__ == other.__class__ and \n",
    "                self.Data == other.Data\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a NetworkX graph for the review network\n",
    "review_network = nx.Graph()\n",
    "\n",
    "# For each review, create a node for the user and business and a link between them\n",
    "for r in reviews.itertuples():\n",
    "    a = Node(r.user_id, ELITE_USER if r.user_id in elite_ids else USER)\n",
    "    b = Node(r.business_id, BIZ)\n",
    "    review_network.add_edge(a, b, weight=r.stars)\n",
    "\n",
    "# Show the number of nodes and edges\n",
    "print('Nodes:', len(review_network.nodes()))\n",
    "print('Edges:', len(review_network.edges()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review network measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate nodes based on their type\n",
    "review_biz_nodes = [n for n in list(review_network.nodes()) if n.Type == BIZ]\n",
    "review_user_nodes = [n for n in list(review_network.nodes()) if n.Type == USER]\n",
    "review_elite_nodes = [n for n in list(review_network.nodes()) if n.Type == ELITE_USER]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regular user degree distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_plot(review_network, review_user_nodes, title=\"all Toronto users\", filename='reviews_degree_normal_users')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elite user distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_plot(review_network, review_elite_nodes, title=\"Toronto Elite users\", filename='reviews_degree_elite_users')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restaurant degree distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_plot(review_network, review_biz_nodes, title=\"Toronto restaurants\", filename='reviews_degree_all_biz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_coeff_avg = nx.average_clustering(review_network)\n",
    "print('Average clustering coeff. :', cluster_coeff_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Centrality measures\n",
    "* Degree centrality is a basic measure for the number of reviews a user has given, and for a restaurant it represents the number of reviews the restaurant has been given.\n",
    "* Eigenvector centrality for a user node indicates to which degree they have reviewed restaurants with many reviews, and for a restaurant the measure represents how many reviews come from users who themselves have given a lot of reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree centrality\n",
    "deg = nx.degree(review_network)\n",
    "deg_elite_user = [deg[n] for n in deg if n.Type == ELITE_USER]\n",
    "deg_user = [deg[n] for n in deg if n.Type == USER]\n",
    "elite_avg_deg = np.mean(deg_elite_user)\n",
    "user_avg_deg = np.mean(deg_user)\n",
    "all_user_deg = np.mean(deg_elite_user + deg_user)\n",
    "\n",
    "# Show results\n",
    "print('Normal user mean degree centrality', user_avg_deg)\n",
    "print('Elite user mean degree centrality', elite_avg_deg)\n",
    "print('All users mean degree centrality', all_user_deg)\n",
    "ratio = elite_avg_deg / user_avg_deg\n",
    "print('Ratio degree (Elite : Normal): %.2f' % ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eigenvalue centrality\n",
    "ev = nx.eigenvector_centrality_numpy(review_network)\n",
    "ev_elite_user = [ev[n] for n in ev if n.Type == ELITE_USER]\n",
    "ev_user = [ev[n] for n in ev if n.Type == USER]\n",
    "elite_avg_ev = np.mean(ev_elite_user)\n",
    "user_avg_ev = np.mean(ev_user)\n",
    "all_user_ev = np.mean(ev_elite_user + ev_user)\n",
    "\n",
    "# Show results\n",
    "print('Normal user mean EV centrality', user_avg_ev)\n",
    "print('Elite user mean EV centrality', elite_avg_ev)\n",
    "print('All users mean EV centrality', all_user_ev)\n",
    "ratio = elite_avg_ev / user_avg_ev\n",
    "print('Ratio EV (Elite : Normal): %.2f' % ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalue centrality vs average user rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column in the user dataframe\n",
    "user['ev'] = 0\n",
    "ev_user = {n.Data: ev[n] for n in ev if (n.Type == ELITE_USER) or (n.Type == USER)}\n",
    "\n",
    "# Insert the eigenvalue of the user in the dataframe. This takes several minutes...\n",
    "i = 1\n",
    "p = int(len(ev_user) / 100)\n",
    "for k in ev_user:\n",
    "    if  i % p == 0: print('%i percent done' % (i/p))\n",
    "    eigenvalue = ev_user[k]\n",
    "    user.loc[user.user_id == k, 'ev'] = eigenvalue\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the eigenvalue of the user vs. the average rating the user\n",
    "plt.figure(figsize=[15,10])\n",
    "plt.scatter(user.average_stars, user.ev, edgecolors='black')\n",
    "plt.xlabel('Yelp average rating')\n",
    "plt.ylabel('Eigenvalue centrality')\n",
    "plt.title('Eigenvalue centrality vs. average user rating for Yelp users in Toronto')\n",
    "plt.savefig('plots/user_rating_ev.svg', format='svg', bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restaurants and eigenvalue centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_biz = {n.Data: ev[n] for n in ev if n.Type == BIZ}\n",
    "deg_biz = {n.Data: deg[n] for n in deg if n.Type == BIZ}\n",
    "\n",
    "biz['ev'] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ev_biz:\n",
    "    eigenvalue = ev_biz[k]\n",
    "    biz.loc[biz.business_id == k, 'ev'] = eigenvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15,10])\n",
    "plt.scatter(biz.stars, biz.ev, edgecolors='black')\n",
    "plt.xlabel('Yelp rating')\n",
    "plt.ylabel('Eigenvalue centrality')\n",
    "plt.title('Eigenvalue centrality vs Yelp rating for restaurants in Toronto')\n",
    "plt.savefig('plots/biz_rating_ev.svg', format='svg', bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15,10])\n",
    "plt.scatter(biz.ev, biz.review_count, edgecolors='black')\n",
    "plt.xlabel('Eigenvector centrality score')\n",
    "plt.ylabel('Review count')\n",
    "plt.title('Eigenvector centrality vs. number of review for restaurants in Toronto')\n",
    "plt.savefig('plots/biz_ev_count.svg', format='svg', bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review differences\n",
    "For this chapter, the differences in ratings between the regular users and the elite users were investigated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall rating distributions\n",
    "Are elite users overall harsher in their reviews? Or is it the other way around? Let us find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elite_stars = np.array(reviews[reviews.user_id.isin(elite_user.user_id)].stars)\n",
    "regular_stars = np.array(reviews[~reviews.user_id.isin(elite_user.user_id)].stars)\n",
    "\n",
    "reg = np.histogram(regular_stars, bins=[1,2,3,4,5,6])[0]\n",
    "reg = reg / sum(reg)\n",
    "\n",
    "elit = np.histogram(elite_stars, bins=[1,2,3,4,5,6])[0]\n",
    "elit = elit / sum(elit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** Elite users are more moderate and peak at 4 stars, where regular users are more critical and over-enthustiastic, i.e. giving 1 star reviews, and 5 star reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concrete differences in ratings\n",
    "Are elite users harsher in their reviews? Or is it the other way around? Let us find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only elite reviews\n",
    "elite_biz_graph = nx.subgraph(review_network, review_elite_nodes + review_biz_nodes)\n",
    "elite_weights_dict = elite_biz_graph.degree(review_biz_nodes, weight='weight')\n",
    "elite_degrees_dict = elite_biz_graph.degree(review_biz_nodes)\n",
    "elite_biz_ratings_dict = {\n",
    "    node.Data: elite_weights_dict[node] / elite_degrees_dict[node]\n",
    "    for node in review_biz_nodes\n",
    "    if elite_degrees_dict[node] > 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All user reviews\n",
    "all_weights_dict = review_network.degree(review_biz_nodes, weight='weight')\n",
    "all_degrees_dict = review_network.degree(review_biz_nodes)\n",
    "all_biz_ratings_dict = {\n",
    "    node.Data: all_weights_dict[node] / all_degrees_dict[node] \n",
    "    for node in review_biz_nodes\n",
    "    if all_degrees_dict[node] > 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison REGULAR AND ELITE\n",
    "biz_ids = [b.Data for b in review_biz_nodes]\n",
    "deltas_reg = {\n",
    "    biz_id: elite_biz_ratings_dict[biz_id] - reg_biz_ratings_dict[biz_id]\n",
    "    for biz_id in biz_ids\n",
    "    if biz_id in reg_biz_ratings_dict.keys()\n",
    "    and biz_id in elite_biz_ratings_dict.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison ALL AND ELITE\n",
    "deltas_all = {\n",
    "    biz_id: elite_biz_ratings_dict[biz_id] - all_biz_ratings_dict[biz_id]\n",
    "    for biz_id in biz_ids\n",
    "    if biz_id in all_biz_ratings_dict.keys()\n",
    "    and biz_id in elite_biz_ratings_dict.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[20,5])\n",
    "plt.hist(100 * np.array(list(deltas_all.values())) / 5, bins=200, edgecolor='black')\n",
    "plt.xlabel('Delta (%)')\n",
    "plt.title('Elite reviews compared to all user reviews')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[20,5])\n",
    "plt.hist(100 * np.array(list(deltas_reg.values())) / 5, bins=200, edgecolor='black')\n",
    "plt.xlabel('Delta (%)')\n",
    "plt.title('Elite reviews compared to regular user reviews')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion. Think critically about your creation\n",
    "### What went well?\n",
    "* Cleaning the dataset\n",
    "* Analysing the social network\n",
    "* Text analysis\n",
    "\n",
    "### What is still missing? What could be improved?\n",
    "* Review network is still kind of inconclusive, we had hoped that a better analysis would come of it\n",
    "* An idea we would like to try, but requires taking a subgraph of the graph is making a review graph based on users who reviewed the same restaurant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
