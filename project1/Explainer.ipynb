{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "### What is your dataset?\n",
    "The dataset used in this project comes from the data provided for the Yelp Dataset Challenge. This dataset consists of about 1.5 million users and about 200 thousand businesses from North America.  Additionally the dataset includes just under 6 million reviews, made by users of the Yelp service, to businesses. The businesses included in the dataset are both restaurants as well as businesses offering other services, such as postal delivery. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why did you choose this/these particular dataset(s)?\n",
    "The dataset is tremendous\n",
    "### What was your goal for the end user's experience?\n",
    "The purpose of this project is to investigate properties of Yelp’s Elite users. For this paper, the focus will lie on Yelp’s two primary claims about their Elite users:\n",
    "\n",
    "Yelp states that its Elite users have high connectivity, which means that they are connected with many other users and interact often with members of their Yelp community. \n",
    "\n",
    "Yelp claims that its Elite users make up the “true heart of the Yelp community.” Third, Yelp claims that its users have high contribution, which means that the user has made a large impact on the site with meaningful and high-quality reviews. \n",
    "\n",
    "The first goal of our project is to analyze whether the above claims about Yelp’s Elite users are quantifiably valid. For this, we will specify several characteristics which we expect Elite users to have based on these claims. We will then perform analyses on Yelp’s dataset in order to determine whether these properties are truly represented among the Elite users. The secondary goal of our project is to find which properties are most indicative of Elite status on Yelp. \n",
    "\n",
    "The analyses for the first goal can be used for this purpose as well. This kind of information may be useful for those who are interested in becoming Elite members on Yelp. In order to become a member of the “Elite squad,” a user must go through an application process. Despite the suggestions presented above, Yelp doesn’t provide any specific criteria on exactly what characteristics a user must have to become Elite. The mystery behind the selection process for Elite users is well-documented.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic stats. Let's understand the dataset better\n",
    "### Write about your choices in data cleaning and preprocessing\n",
    "- Mis-formatted JSON to valid JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def cleanup(k, dataset, chunk_size):\n",
    "    dirty_path = 'yelp_dataset/yelp_academic_dataset_%s.json' % dataset\n",
    "    clean_path = \"cleaned/%s%i.json\" % (dataset, k)\n",
    "    dirty_file = open(dirty_path, \"r\")\n",
    "    clean_file = open(clean_path, \"w\")\n",
    "    \n",
    "    start = chunk_size * k\n",
    "    end = chunk_size * (k+1)\n",
    "    \n",
    "    content = ''\n",
    "    i = 0\n",
    "    for line in dirty_file:\n",
    "        if i == end:\n",
    "            break\n",
    "        elif i >= start:\n",
    "            s = line.replace('\\n', ',\\n')\n",
    "            content += s\n",
    "        i += 1\n",
    "    if content:\n",
    "        payload = '{\"data\" : \\n[%s]}' % (content[:-2] + '\\n')\n",
    "        clean_file.write(payload)\n",
    "    else:\n",
    "        print(\"No more content.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all restaurants from Toronto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in JSON data, convert to a dataframe\n",
    "json_data = list(pd.read_json('cleaned/business.json').data)\n",
    "df = pd.DataFrame(json_data)\n",
    "\n",
    "# Restaurants will contain the keywords 'restaurant' \n",
    "# and/or 'food' in the 'category' attribute.\n",
    "keywords = ['restaurant', 'food']\n",
    "idx = df.categories.str.lower().str.contains(\"|\".join(keywords)).fillna(False)\n",
    "rest = df[idx]\n",
    "\n",
    "# Drop attributes irrelevant to the analysis\n",
    "rest = rest.drop(['attributes', 'categories', 'address', 'neighborhood'], axis=1)\n",
    "\n",
    "# Save dataset to CSV\n",
    "rest.to_csv('cleaned_csv/restaurants.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biz = pd.read_csv('cleaned_csv/restaurants.csv').drop(['is_open', 'hours', 'Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all reviews from Toronto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all users in the Toronto reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(20):\n",
    "    chunk_size = 100 * 1000\n",
    "    cleanup(k, 'user', chunk_size)\n",
    "    print('Iteration', k, 'done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 16\n",
    "df_matrix = [None] * N\n",
    "for i in range(N):\n",
    "    path = \"cleaned/user%i.json\" % i\n",
    "    df_matrix[i] = pd.DataFrame(list(pd.read_json(path).data))\n",
    "\n",
    "DF = pd.concat(df_matrix)\n",
    "reviews = pd.read_csv('toronto/reviews_all_time.csv')\n",
    "\n",
    "# Filter out users not in the Toronto reviews\n",
    "toronto_users = DF[DF.user_id.isin(reviews.user_id)]\n",
    "\n",
    "# Throw out unused columns, to save space\n",
    "toronto_users = toronto_users.drop(['compliment_cool', 'compliment_cute',\n",
    "       'compliment_funny', 'compliment_hot', 'compliment_list',\n",
    "       'compliment_more', 'compliment_note', 'compliment_photos',\n",
    "       'compliment_plain', 'compliment_profile', 'compliment_writer', 'cool',\n",
    "     'funny', 'fans'], axis=1)\n",
    "\n",
    "# Save to CSV\n",
    "toronto_users.to_csv('toronto/toronto_users.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all restaurants in Toronto\n",
    "biz = pd.read_csv('cleaned_csv/business.csv').drop(['is_open', 'hours', 'Unnamed: 0'], axis=1)\n",
    "biz.city = biz.city.str.lower()\n",
    "biz_toronto = biz[biz.city == 'toronto']\n",
    "print(\"Restaurants in Toronto: \", biz_toronto.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make \n",
    "n = 6\n",
    "df_list = [None] * n\n",
    "for i in range(n):\n",
    "    path = \"cleaned/review%i.json\" % i\n",
    "    df_list[i] = pd.DataFrame(list(pd.read_json(path).data))    \n",
    "\n",
    "DF = pd.concat(df_list)\n",
    "reviews = pd.read_csv('toronto/reviews_all_time.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- JSON to DataFrame, to CSV (takes up less space)\n",
    "- Select only those businesses which are located in Toronto\n",
    "- Filter out non-restaurants\n",
    "- Use the dataframe of Toronto restaurants to identify reviews made by a user, reviewing a Toronto restaurants\n",
    "- Use the list of Toronto reviews to filter out users who have not contributed to the Toronto Yelp reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a short section that discusses the dataset stats (here you can recycle the work you did for Project Assignment A)\n",
    "- Reviews: ~6 million\n",
    "- Users: Many\n",
    "- Businesses: ~200,000\n",
    "\n",
    "For this project the restaurants in Toronto were the main focus, as Toronto is a big city with more than a sufficient amount of data to perform a serious analysis, but small enough for various graph algorithms to be carried out. The users considered in this project were all the users who left a review on a business in Toronto.\n",
    "\n",
    "- Period: March 1st 2008 to August 1st 2018\n",
    "- Reviews: ~380,000\n",
    "- Users: ~85,000\n",
    "- Elite users hereof: ~7,500\n",
    "- Restaurants: ~10,000\n",
    "\n",
    "This is analogous to the paper which had 250k nodes and 950k edges, where both are just scaled up by around a factor of 2.5!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools, theory and analysis. Describe the process of theory to insight\n",
    "### Talk about how you've worked with text, including regular expressions, unicode, etc.\n",
    "### Describe which network science tools and data analysis strategies you've used, how those network science measures work, and why the tools you've chosen are right for the problem you're solving.\n",
    "#### Modelling the network\n",
    "The Toronto Yelp review network was modelled as an undirected graph, containing user nodes where the edges between two user nodes represent the fact two users have reviewed the same restaurant. \n",
    "\n",
    "#### Most connected subcomponent\n",
    "Detecting how important the elite users were for the network was done by deleting them one by one from the graph, and then watching how the largest connected subgraph shrinks. The elite users were deleted based on their degree centrality.\n",
    "### How did you use the tools to understand your dataset?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion. Think critically about your creation\n",
    "### What went well?\n",
    "### What is still missing? What could be improved?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
