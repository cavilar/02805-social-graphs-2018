{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "### What is your dataset?\n",
    "The dataset used in this project comes from the data provided for the Yelp Dataset Challenge. This dataset consists of about 1.5 million users and about 200 thousand businesses from North America.  Additionally the dataset includes just under 6 million reviews, made by users of the Yelp service, to businesses. The businesses included in the dataset are both restaurants as well as businesses offering other services, such as postal delivery. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why did you choose this/these particular dataset(s)?\n",
    "The dataset is tremendous\n",
    "### What was your goal for the end user's experience?\n",
    "The purpose of this project is to investigate properties of Yelp’s Elite users. For this paper, the focus will lie on Yelp’s two primary claims about their Elite users:\n",
    "\n",
    "Yelp states that its Elite users have high connectivity, which means that they are connected with many other users and interact often with members of their Yelp community. \n",
    "\n",
    "Yelp claims that its Elite users make up the “true heart of the Yelp community.” Third, Yelp claims that its users have high contribution, which means that the user has made a large impact on the site with meaningful and high-quality reviews. \n",
    "\n",
    "The first goal of our project is to analyze whether the above claims about Yelp’s Elite users are quantifiably valid. For this, we will specify several characteristics which we expect Elite users to have based on these claims. We will then perform analyses on Yelp’s dataset in order to determine whether these properties are truly represented among the Elite users. The secondary goal of our project is to find which properties are most indicative of Elite status on Yelp. \n",
    "\n",
    "The analyses for the first goal can be used for this purpose as well. This kind of information may be useful for those who are interested in becoming Elite members on Yelp. In order to become a member of the “Elite squad,” a user must go through an application process. Despite the suggestions presented above, Yelp doesn’t provide any specific criteria on exactly what characteristics a user must have to become Elite. The mystery behind the selection process for Elite users is well-documented.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic stats. Let's understand the dataset better\n",
    "### Write about your choices in data cleaning and preprocessing\n",
    "- Mis-formatted JSON to valid JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def cleanup(N, dataset, chunk_size=100000):\n",
    "    '''\n",
    "    Cleans up a JSON file by adding a trailing comma to each line,\n",
    "    which is missing from the Yelp dataset files.\n",
    "    A chunk size must be specified, since all the lines in the data\n",
    "    files cannot be stored in memory at the same time, due to being very large!\n",
    "    '''\n",
    "    for k in range(N):\n",
    "        dirty_path = 'yelp_dataset/yelp_academic_dataset_%s.json' % dataset\n",
    "        clean_path = \"cleaned/%s%i.json\" % (dataset, k)\n",
    "        dirty_file = open(dirty_path, \"r\")\n",
    "        clean_file = open(clean_path, \"w\")\n",
    "\n",
    "\n",
    "        start = chunk_size * k\n",
    "        end = chunk_size * (k+1)\n",
    "\n",
    "        content = ''\n",
    "        i = 0\n",
    "        for line in dirty_file:\n",
    "            if i == end:\n",
    "                break\n",
    "            elif i >= start:\n",
    "                s = line.replace('\\n', ',\\n')\n",
    "                content += s\n",
    "            i += 1\n",
    "        if content:\n",
    "            payload = '{\"data\" : \\n[%s]}' % (content[:-2] + '\\n')\n",
    "            clean_file.write(payload)\n",
    "        else:\n",
    "            print(\"No more content.\")\n",
    "    \n",
    "    print('Iteration', k, 'done')\n",
    "    \n",
    "def read_json_to_df(N, dataset):\n",
    "    # Create dataframe from JSON files\n",
    "    df_matrix = [None] * N\n",
    "    for i in range(N):\n",
    "        path = \"cleaned/business%i.json\" % i\n",
    "        df_matrix[i] = pd.DataFrame(list(pd.read_json(path).data))\n",
    "    return pd.concat(df_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all restaurants from Toronto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tnni/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py:4405: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "# Clean business JSON files\n",
    "N = 2 # There are about 200k restaurants, therefore 2 chunks of 100k elements is sufficient\n",
    "dataset = 'business'\n",
    "cleanup(N, dataset)\n",
    "\n",
    "# Make dataframe from JSON data\n",
    "df = read_json_to_df(N, dataset)\n",
    "\n",
    "# Restaurants will contain the keywords 'restaurant' \n",
    "# and/or 'food' in the 'category' attribute.\n",
    "keywords = ['restaurant', 'food']\n",
    "idx = df.categories.str.lower().str.contains(\"|\".join(keywords)).fillna(False)\n",
    "rest = df[idx]\n",
    "\n",
    "\n",
    "# Only include Toronto restaurants\n",
    "rest.city = rest.city.str.lower()\n",
    "rest = rest[rest.city == 'toronto']\n",
    "\n",
    "# Drop attributes irrelevant to the analysis\n",
    "rest = rest.drop(['city', 'attributes', 'categories', 'address', 'neighborhood', 'is_open', 'hours'], axis=1)\n",
    "\n",
    "# Save dataset to CSV\n",
    "rest.to_csv('toronto2/toronto_restaurants.csv', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all reviews from Toronto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-95a5ea671fac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m60\u001b[0m \u001b[0;31m# There are about 6M restaurants, therefore 60 chunks of 100k elements is sufficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'review'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Make dataframe from JSON data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-3786eed98abe>\u001b[0m in \u001b[0;36mcleanup\u001b[0;34m(N, dataset, chunk_size)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdirty_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Clean business JSON files\n",
    "N = 60 # There are about 6M reviews, therefore 60 chunks of 100k elements is sufficient\n",
    "dataset = 'review'\n",
    "cleanup(N, dataset)\n",
    "\n",
    "# Make dataframe from JSON data\n",
    "df = read_json_to_df(N, dataset)\n",
    "\n",
    "# Filter out reviews of businesses outside Toronto\n",
    "reviews = df[df.business_id.isin(rest.business_id)]\n",
    "\n",
    "# Drop attributes irrelevant to the analysis\n",
    "reviews = reviews.drop(['cool', 'funny', 'useful'], axis=1)\n",
    "\n",
    "# Save dataset to CSV\n",
    "reviews.to_csv('toronto2/toronto_reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all users in the Toronto reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean business JSON files\n",
    "N = 30 # A guess\n",
    "dataset = 'user'\n",
    "cleanup(N, dataset)\n",
    "\n",
    "# Make dataframe from JSON data\n",
    "df = read_json_to_df(N, dataset)\n",
    "\n",
    "# Filter out users not in the Toronto reviews\n",
    "toronto_users = df[df.user_id.isin(reviews.user_id)]\n",
    "\n",
    "# Drop attributes irrelevant to the analysis\n",
    "toronto_users = toronto_users.drop(['compliment_cool', 'compliment_cute',\n",
    "       'compliment_funny', 'compliment_hot', 'compliment_list',\n",
    "       'compliment_more', 'compliment_note', 'compliment_photos',\n",
    "       'compliment_plain', 'compliment_profile', 'compliment_writer', 'cool',\n",
    "     'funny', 'fans'], axis=1)\n",
    "\n",
    "# Save to CSV\n",
    "toronto_users.to_csv('toronto/toronto_users.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset stats\n",
    "- Reviews: ~6 million\n",
    "- Users: Many\n",
    "- Businesses: ~200,000\n",
    "\n",
    "For this project the restaurants in Toronto were the main focus, as Toronto is a big city with more than a sufficient amount of data to perform a serious analysis, but small enough for various graph algorithms to be carried out. The users considered in this project were all the users who left a review on a business in Toronto.\n",
    "\n",
    "- Period: March 1st 2008 to August 1st 2018\n",
    "- Reviews: ~380,000\n",
    "- Users: ~85,000\n",
    "- Elite users hereof: ~7,500\n",
    "- Restaurants: ~10,000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools, theory and analysis. Describe the process of theory to insight\n",
    "### Talk about how you've worked with text, including regular expressions, unicode, etc.\n",
    "### Describe which network science tools and data analysis strategies you've used, how those network science measures work, and why the tools you've chosen are right for the problem you're solving.\n",
    "#### Modelling the network\n",
    "The Toronto Yelp review network was modelled as an undirected graph, containing user nodes where the edges between two user nodes represent the fact two users have reviewed the same restaurant. \n",
    "\n",
    "#### Most connected subcomponent\n",
    "Detecting how important the elite users were for the network was done by deleting them one by one from the graph, and then watching how the largest connected subgraph shrinks. The elite users were deleted based on their degree centrality.\n",
    "### How did you use the tools to understand your dataset?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion. Think critically about your creation\n",
    "### What went well?\n",
    "### What is still missing? What could be improved?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
