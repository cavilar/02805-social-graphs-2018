{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import nltk, re, pprint, io, json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_query_url(page):\n",
    "    # Build query\n",
    "    queryUrl = \"http://en.wikipedia.org/w/api.php/?action=query\"\n",
    "    title = \"titles=%s\" % page \n",
    "    content = \"prop=extracts&exlimit=max&explaintext\"\n",
    "    dataformat = \"format=json\"\n",
    "    query = \"%s&%s&%s&%s\" % (queryUrl, title, content, dataformat)\n",
    "    return query\n",
    "\n",
    "def get_content(url):\n",
    "    # Send request and parse response\n",
    "    json_response = requests.get(url).json()\n",
    "    pages = json_response['query']['pages']\n",
    "    key = next(iter(pages.keys()))\n",
    "    content = pages[key]['extract']\n",
    "    return content\n",
    "    \n",
    "def fetch_content(page):\n",
    "    url = build_query_url(page)\n",
    "    content = get_content(url)\n",
    "    return content\n",
    "\n",
    "def save_to_file(content, page_name):\n",
    "    filename = 'congress115/%s.txt' % page_name\n",
    "    f = open(filename, \"a\")\n",
    "    f.write(content)\n",
    "    f.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe which contains page names for the 115th congress\n",
    "url_h115 = 'https://raw.githubusercontent.com/suneman/socialgraphs2018/master/files/data_US_congress/H115.csv'\n",
    "df = pd.read_csv(url_h115)\n",
    "page_names = df.WikiPageName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.9 s, sys: 1.46 s, total: 17.4 s\n",
      "Wall time: 3min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Fetch each wikipage and save to a txt file\n",
    "for page_name in page_names:\n",
    "    content = fetch_content(page_name)\n",
    "    save_to_file(content, page_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "### TF-IDF\n",
    "**Explain in your own words the point of TF-IDF.**\n",
    "* What does TF stand for?\n",
    "* What does IDF stand for?\n",
    "\n",
    "Answer:\n",
    "* TFIDF (term frequency–inverse document frequency), is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. Tf–idf is one of the most popular term-weighting schemes today.\n",
    "\n",
    "* Variations of the tf–idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query. tf–idf can be successfully used for stop-words filtering in various subject fields, including text summarization and classification.\n",
    "\n",
    "### Tokenizing the Wikipedia Pages\n",
    "We want to find out which words are important for each party, so we're going to create two large documents, one for the Democratic and one for the Republican party. Tokenize the pages, and combine the tokens into one long list including all the pages of the members of the same party. Remember the bullets below for success.\n",
    "* Exclude the congress members names (since we're interested in the words, not the names).\n",
    "* Exclude punctuation.\n",
    "* Exclude stop words (if you don't know what stop words are, go back and read NLPP1e again).\n",
    "* Exclude numbers (since they're difficult to interpret in the word cloud).\n",
    "* Set everything to lower case.\n",
    "\n",
    "*Note that none of the above has to be perfect. It might not be easy to remove all representatives names. And there's some room for improvisation. You can try using stemming. In my own first run the results didn't look so nice, because some pages are very detailed and repeat certain words again and again and again, whereas other pages are very short. For that reason, I decided to use the unique set of words from each page rather than each word in proportion to how it's actually used on that page. Choices like that are up to you.\n",
    "Now, we're ready to calculate the TF for each word. Use the method of your choice to find the top 5 terms within each party.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "all_words = \" \".join(page_names)\n",
    "all_tokens = nltk.word_tokenize(all_words)\n",
    "\n",
    "# Only take words with a capital start letter\n",
    "# Replace underscores with whitespaces\n",
    "# Remove the word \"politician\" from the names\n",
    "names = [\n",
    "    name.replace(\"_\", \" \").replace(\"politician\", \"\") \n",
    "    for name in all_tokens \n",
    "    if name[0] in string.ascii_uppercase\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = \"\\n\".join(names)\n",
    "f = open(\"member_names_pretty.txt\", \"w\")\n",
    "f.write(out)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_document(document, unwanted_sentences):\n",
    "    '''\n",
    "    INPUT: document (String), unwanted_sentences (String list)\n",
    "    OUTPUT: stems (String list)\n",
    "    '''\n",
    "    # Remove bad sentences, ex full names from document\n",
    "    for s in unwanted_sentences:\n",
    "        document = document.replace(s, \"\")\n",
    "    \n",
    "    # Tokenize all words (no digits, no punctuation)\n",
    "    tokens = re.findall(r'[a-zA-Z]+', document)\n",
    "    \n",
    "    # Stemming    \n",
    "    stemmer = nltk.LancasterStemmer()\n",
    "    tokens = [stemmer.stem(t) for t in tokens]\n",
    "    \n",
    "    # Stop-word filtering\n",
    "    tokens = [\n",
    "        word for word in tokens \n",
    "        if word not in stopwords.words('english')\n",
    "    ]\n",
    "    \n",
    "    return tokens  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_token_matrix(party):\n",
    "    token_matrix = []\n",
    "    names = df[df.Party == party].WikiPageName\n",
    "    for name in names:\n",
    "        document = io.open('congress115/%s.txt' % name, 'r').read()\n",
    "        cleaned = clean_document(document, page_names)\n",
    "        token_matrix.append(cleaned)\n",
    "    return token_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "### !!! TAKES A LONG TIME!!! :( !!!\n",
    "republican_token_matrix = build_token_matrix(\"Republican\")\n",
    "democratic_token_matrix = build_token_matrix(\"Democratic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Serialization\n",
    "import pickle\n",
    "filehandler = open(\"republican_token_matrix.obj\",\"wb\")\n",
    "pickle.dump(republican_token_matrix, filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "filehandler = open(\"democratic_token_matrix.obj\",\"wb\")\n",
    "pickle.dump(democratic_token_matrix, filehandler)\n",
    "filehandler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"republican_token_matrix.obj\",'rb')\n",
    "republican_token_matrix = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open(\"democratic_token_matrix.obj\",'rb')\n",
    "democratic_token_matrix = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Flatten matrices\n",
    "republican_tokens = [word for document in republican_token_matrix for word in document]\n",
    "democratic_tokens = [word for document in democratic_token_matrix for word in document]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now, we're ready to calculate the TF for each word. Use the method of your choice to find the top 5 terms within each party.\n",
    "\n",
    "For TF I will be using the simplest variation, i.e. just the frequency of the word.\n",
    "\n",
    "$tf(t,d) = f_{t,d}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "republican_fd = nltk.FreqDist(republican_tokens)\n",
    "democratic_fd = nltk.FreqDist(democratic_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('stat', 3464),\n",
       " ('congress', 3083),\n",
       " ('vot', 3029),\n",
       " ('elect', 2896),\n",
       " ('hous', 2603)]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "republican_fd.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('congress', 3003),\n",
       " ('stat', 2557),\n",
       " ('elect', 2483),\n",
       " ('hous', 2062),\n",
       " ('democr', 1970)]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "democratic_fd.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting TF's\n",
    "tf_repub = list(\n",
    "    map(lambda tup: tup[1], republican_fd.most_common())\n",
    ")\n",
    "\n",
    "tf_demo = list(\n",
    "    map(lambda tup: tup[1], democratic_fd.most_common())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(<map object at 0x1377669b0>, dtype=object)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Next, we calculate IDF for every word.\n",
    "  * What base logarithm did you use? Is that important?\n",
    "  \n",
    "$idf(t,D) = log \\left( \\frac{N}{df_t} \\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "republican_most_common_ordered = list(\n",
    "    map(lambda tup: tup[0], republican_fd.most_common())\n",
    ")\n",
    "\n",
    "democratic_most_common_ordered = list(\n",
    "    map(lambda tup: tup[0], democratic_fd.most_common())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n_repub = len(republican_most_common_ordered)\n",
    "n_demo = len(democratic_most_common_ordered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_republican = np.zeros(n_repub)\n",
    "for i, token in enumerate(republican_most_common_ordered):\n",
    "    for document_tokens in republican_token_matrix:\n",
    "        if token in document_tokens: \n",
    "            df_republican[i] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_democratic = np.zeros(n_demo)\n",
    "for i, token in enumerate(democratic_most_common_ordered):\n",
    "    for document_tokens in democratic_token_matrix:\n",
    "        if token in document_tokens: \n",
    "            df_democratic[i] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we are able to calculate the idf's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Republican\n",
    "idf_repub = np.log(n_repub / df_republican)\n",
    "\n",
    "# Democratic\n",
    "idf_demo = np.log(n_demo / df_democratic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculating $TF \\cdot IDF$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_repub = np.array(tf_repub)\n",
    "tf_demo = np.array(tf_demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_repub = np.multiply(tf_repub, idf_repub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11038"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_repub.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.31482023e+04, 1.17020518e+04, 1.15093240e+04, ...,\n",
       "       9.30909914e+00, 9.30909914e+00, 9.30909914e+00])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.multiply(tf_repub, idf_repub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  6, 20, 42, 72])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.multiply(np.arange(1,10,2), np.arange(0,9,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
