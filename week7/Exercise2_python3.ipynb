{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import nltk, re, pprint, io, json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_content(page):\n",
    "    def build_query_url(page):\n",
    "        # Build query\n",
    "        queryUrl = \"http://en.wikipedia.org/w/api.php/?action=query\"\n",
    "        title = \"titles=%s\" % page \n",
    "        content = \"prop=extracts&exlimit=max&explaintext\"\n",
    "        rvprop= \"rvprop=timestamp|content\"\n",
    "        dataformat = \"format=json\"\n",
    "        query = \"%s&%s&%s&%s&%s\" % (queryUrl, title, content, rvprop, dataformat)\n",
    "        return query\n",
    "    \n",
    "    def get_content(url):\n",
    "        # Send request and parse response\n",
    "        json_response = requests.get(u).json()\n",
    "        pages = json_response['query']['pages']\n",
    "        key = next(iter(ps.keys()))\n",
    "        content = ps[key]['extract']\n",
    "        return content\n",
    "    \n",
    "    url = build_query_url(page)\n",
    "    content = get_content(url)\n",
    "    return content\n",
    "\n",
    "def save_to_file(content, page_name):\n",
    "    filename = 'congress115/%s.txt' % page_name\n",
    "    f = open(filename, \"a\")\n",
    "    f.write(content)\n",
    "    f.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe which contains page names for the 115th congress\n",
    "url_h115 = 'https://raw.githubusercontent.com/suneman/socialgraphs2018/master/files/data_US_congress/H115.csv'\n",
    "df = pd.read_csv(url_h115)\n",
    "page_names = df.WikiPageName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.8 s, sys: 1.51 s, total: 17.3 s\n",
      "Wall time: 3min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Fetch each wikipage and save to a txt file\n",
    "for page_name in page_names:\n",
    "    content = fetch_content(page_name)\n",
    "    save_to_file(content, page_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "### TF-IDF\n",
    "**Explain in your own words the point of TF-IDF.**\n",
    "* What does TF stand for?\n",
    "* What does IDF stand for?\n",
    "\n",
    "Answer:\n",
    "* TFIDF (term frequency–inverse document frequency), is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. Tf–idf is one of the most popular term-weighting schemes today.\n",
    "\n",
    "* Variations of the tf–idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query. tf–idf can be successfully used for stop-words filtering in various subject fields, including text summarization and classification.\n",
    "\n",
    "### Tokenizing the Wikipedia Pages\n",
    "We want to find out which words are important for each party, so we're going to create two large documents, one for the Democratic and one for the Republican party. Tokenize the pages, and combine the tokens into one long list including all the pages of the members of the same party. Remember the bullets below for success.\n",
    "* Exclude the congress members names (since we're interested in the words, not the names).\n",
    "* Exclude punctuation.\n",
    "* Exclude stop words (if you don't know what stop words are, go back and read NLPP1e again).\n",
    "* Exclude numbers (since they're difficult to interpret in the word cloud).\n",
    "* Set everything to lower case.\n",
    "\n",
    "*Note that none of the above has to be perfect. It might not be easy to remove all representatives names. And there's some room for improvisation. You can try using stemming. In my own first run the results didn't look so nice, because some pages are very detailed and repeat certain words again and again and again, whereas other pages are very short. For that reason, I decided to use the unique set of words from each page rather than each word in proportion to how it's actually used on that page. Choices like that are up to you.\n",
    "Now, we're ready to calculate the TF for each word. Use the method of your choice to find the top 5 terms within each party.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = io.open(\"congress115/Adam_Kinzinger.txt\", 'r').read().encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
